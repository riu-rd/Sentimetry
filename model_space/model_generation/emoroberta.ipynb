{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "# HuggingFace\n",
    "from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline\n",
    "\n",
    "# Datasets Folder\n",
    "datasets_folder = Path(\"../datasets/goemotions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Emotion Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes: 28\n",
      "['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "classes = urllib.request.urlopen('https://raw.githubusercontent.com/google-research/google-research'\n",
    "'/master/goemotions/data/emotions.txt').read().decode('utf8').split('\\n')\n",
    "num_classes = len(classes)\n",
    "print(f\"Number of Classes: {num_classes}\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoEmotions 1:  (70000, 37)\n",
      "GoEmotions 2:  (70000, 37)\n",
      "GoEmotions 3:  (71225, 37)\n",
      "- - - - - - - - - - - - - - - - - - -\n",
      "GoEmotions Concatenated:  (211225, 37)\n"
     ]
    }
   ],
   "source": [
    "goemotions_1 = pd.read_csv(datasets_folder / \"goemotions_1.csv\")\n",
    "goemotions_2 = pd.read_csv(datasets_folder / \"goemotions_2.csv\")\n",
    "goemotions_3 = pd.read_csv(datasets_folder / \"goemotions_3.csv\")\n",
    "print(\"GoEmotions 1: \", goemotions_1.shape)\n",
    "print(\"GoEmotions 2: \", goemotions_2.shape)\n",
    "print(\"GoEmotions 3: \", goemotions_3.shape)\n",
    "\n",
    "# Concatenate all of the datasets\n",
    "goemotions = pd.concat([goemotions_1, goemotions_2, goemotions_3])\n",
    "print(\"- - - - - - - - - - - - - - - - - - -\\nGoEmotions Concatenated: \", goemotions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>example_very_unclear</th>\n",
       "      <th>admiration</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>eew5j0j</td>\n",
       "      <td>Brdd9</td>\n",
       "      <td>nrl</td>\n",
       "      <td>t3_ajis4z</td>\n",
       "      <td>t1_eew18eq</td>\n",
       "      <td>1.548381e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;sexuality shouldn’t be a grouping category I...</td>\n",
       "      <td>eemcysk</td>\n",
       "      <td>TheGreen888</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>1.548084e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>ed2mah1</td>\n",
       "      <td>Labalool</td>\n",
       "      <td>confessions</td>\n",
       "      <td>t3_abru74</td>\n",
       "      <td>t1_ed2m7g7</td>\n",
       "      <td>1.546428e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>eeibobj</td>\n",
       "      <td>MrsRobertshaw</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>1.547965e+09</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>eda6yn6</td>\n",
       "      <td>American_Fascist713</td>\n",
       "      <td>starwarsspeculation</td>\n",
       "      <td>t3_ackt2f</td>\n",
       "      <td>t1_eda65q2</td>\n",
       "      <td>1.546669e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id  \\\n",
       "0                                    That game hurt.  eew5j0j   \n",
       "1   >sexuality shouldn’t be a grouping category I...  eemcysk   \n",
       "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
       "3                                 Man I love reddit.  eeibobj   \n",
       "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
       "\n",
       "                author            subreddit    link_id   parent_id  \\\n",
       "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
       "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
       "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
       "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
       "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
       "\n",
       "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
       "0  1.548381e+09         1                 False           0  ...     0   \n",
       "1  1.548084e+09        37                  True           0  ...     0   \n",
       "2  1.546428e+09        37                 False           0  ...     0   \n",
       "3  1.547965e+09        18                 False           0  ...     1   \n",
       "4  1.546669e+09         2                 False           0  ...     0   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        0  \n",
       "1         0        0  \n",
       "2         0        1  \n",
       "3         0        0  \n",
       "4         0        1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the Dataset\n",
    "goemotions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate EmoRoBERTa Tokenizer, Model, and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at arpanghoshal/EmoRoBERTa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at arpanghoshal/EmoRoBERTa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Get the EmoRoBERTa from HuggingFace\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"arpanghoshal/EmoRoBERTa\")\n",
    "model = TFRobertaForSequenceClassification.from_pretrained(\"arpanghoshal/EmoRoBERTa\")\n",
    "emotion = pipeline('sentiment-analysis', model='arpanghoshal/EmoRoBERTa', top_k=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the Pretrained Model EmoRoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'label': 'love', 'score': 6.231944722414482},\n",
       "  {'label': 'admiration', 'score': 2.246449695812771},\n",
       "  {'label': 'joy', 'score': 2.210572298936313},\n",
       "  {'label': 'neutral', 'score': 2.0135865128822843},\n",
       "  {'label': 'desire', 'score': 1.2290927699887106},\n",
       "  {'label': 'optimism', 'score': 0.9187107055731758},\n",
       "  {'label': 'gratitude', 'score': 0.833778717849782},\n",
       "  {'label': 'approval', 'score': 0.4793453911697725},\n",
       "  {'label': 'caring', 'score': 0.24826980605575955},\n",
       "  {'label': 'amusement', 'score': 0.15367175083520124},\n",
       "  {'label': 'pride', 'score': 0.14663361136626918},\n",
       "  {'label': 'grief', 'score': 0.062015252020501066},\n",
       "  {'label': 'embarrassment', 'score': 0.04561865358846262},\n",
       "  {'label': 'disgust', 'score': 0.024121032478433335},\n",
       "  {'label': 'remorse', 'score': 0.022114704694104148},\n",
       "  {'label': 'anger', 'score': 0.021033011355029885},\n",
       "  {'label': 'disapproval', 'score': 0.019088391250988934},\n",
       "  {'label': 'relief', 'score': 0.017980166965571698},\n",
       "  {'label': 'excitement', 'score': 0.016105290975247044},\n",
       "  {'label': 'curiosity', 'score': 0.013169958555408812},\n",
       "  {'label': 'confusion', 'score': 0.012984241305730393},\n",
       "  {'label': 'annoyance', 'score': 0.00841981011126336},\n",
       "  {'label': 'realization', 'score': 0.005930580882704817},\n",
       "  {'label': 'fear', 'score': 0.005348332102585118},\n",
       "  {'label': 'sadness', 'score': 0.005184285054838256},\n",
       "  {'label': 'nervousness', 'score': 0.004321519596715007},\n",
       "  {'label': 'disappointment', 'score': 0.00232119538304687},\n",
       "  {'label': 'surprise', 'score': 0.0021880966442040517}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "texts = \"\"\"\n",
    "My Dearest [Partner's Name],\n",
    "\n",
    "As I sit down to write this letter, my heart is overflowing with the love that I hold for you. Words alone cannot express the depth of my feelings, but I hope that these humble words can convey a fraction of the love that resides within me.\n",
    "\n",
    "From the moment I met you, my life has been filled with an abundance of joy and happiness. Your presence brings light into my world, and your laughter is like music to my ears. Every moment spent with you is a treasure, and I am grateful for each second we share together.\n",
    "\n",
    "Your kindness, compassion, and unwavering support have touched my soul in ways I never thought possible. You are my rock, my confidant, and my greatest source of strength. In your embrace, I find solace and comfort, knowing that I am loved unconditionally.\n",
    "\n",
    "With each passing day, my love for you grows stronger and deeper. You are the one I want to share my hopes, dreams, and aspirations with. You are the one I want to build a future with, filled with laughter, adventure, and endless love.\n",
    "\n",
    "As I pen these words, I want you to know that you are cherished beyond measure. You are the most beautiful soul I have ever known, and I am endlessly grateful to have you in my life.\n",
    "\n",
    "I love you more than words can say, and I will spend the rest of my days showing you just how much you mean to me.\n",
    "\n",
    "Forever and always,\n",
    "[Your Name]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def predict_emotions(texts):\n",
    "    # Split the huge chunk of text into a string list\n",
    "    text_list = re.split(r'[.!?;\\n]', texts)\n",
    "    text_list = [text.strip() for text in text_list if text.strip()]\n",
    "\n",
    "    # Create a list of all predictions per text\n",
    "    predictions_per_text = []\n",
    "    for text in text_list:\n",
    "        predictions_per_text.append(emotion(text)[0])\n",
    "    \n",
    "    # Create a defaultdict to aggregate scores for each label\n",
    "    total = defaultdict(float)\n",
    "\n",
    "    # Iterate over each list and aggregate the scores\n",
    "    for prediction in predictions_per_text:\n",
    "        for emotion_dict in prediction:\n",
    "            label = emotion_dict['label']\n",
    "            score = emotion_dict['score']\n",
    "            total[label] += score\n",
    "    \n",
    "    # Convert the defaultdict to a list of dictionaries\n",
    "    result = [{'label': label, 'score': score} for label, score in total.items()]\n",
    "    # Sort the result in descending order\n",
    "    sorted_result = sorted(result, key=lambda x: x['score'], reverse=True)\n",
    "    return {\"predictions\" : sorted_result}\n",
    "\n",
    "predict_emotions(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 'Emotion: grief (76.9)%'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_emotions(text):\n",
    "    prediction = emotion(text)[0]\n",
    "    sorted_prediction = sorted(prediction, key=lambda x: x['score'], reverse=True)\n",
    "    predicted_emotion = sorted_prediction[0]['label']\n",
    "    probability = round(sorted_prediction[0]['score'] * 100, 1)\n",
    "    return {\"prediction\":f\"Emotion: {predicted_emotion} ({probability})%\"}\n",
    "\n",
    "predict_emotions(\"I am sorry for your loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'grief', 'score': 0.7692716121673584}, {'label': 'caring', 'score': 0.12554031610488892}, {'label': 'remorse', 'score': 0.051490552723407745}, {'label': 'sadness', 'score': 0.03515668585896492}, {'label': 'anger', 'score': 0.002508923877030611}, {'label': 'amusement', 'score': 0.002142573706805706}, {'label': 'love', 'score': 0.0014326514210551977}, {'label': 'optimism', 'score': 0.001427307608537376}, {'label': 'fear', 'score': 0.001413228688761592}, {'label': 'gratitude', 'score': 0.001334708882495761}, {'label': 'neutral', 'score': 0.0011805427493527532}, {'label': 'joy', 'score': 0.0009956690482795238}, {'label': 'admiration', 'score': 0.0009573725401423872}, {'label': 'disappointment', 'score': 0.0006602227222174406}, {'label': 'desire', 'score': 0.0006382398423738778}, {'label': 'realization', 'score': 0.0006138188182376325}, {'label': 'pride', 'score': 0.0006107561057433486}, {'label': 'disgust', 'score': 0.0004663134168367833}, {'label': 'relief', 'score': 0.0004287412448320538}, {'label': 'surprise', 'score': 0.0003539887838996947}, {'label': 'curiosity', 'score': 0.0003443522145971656}, {'label': 'excitement', 'score': 0.00018582484335638583}, {'label': 'disapproval', 'score': 0.00017574240337125957}, {'label': 'embarrassment', 'score': 0.00015976901340764016}, {'label': 'nervousness', 'score': 0.0001485338871134445}, {'label': 'confusion', 'score': 0.00014650901721324772}, {'label': 'approval', 'score': 0.00011516744416439906}, {'label': 'annoyance', 'score': 9.984074858948588e-05}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "API_TOKEN = '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'\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/arpanghoshal/EmoRoBERTa\"\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\tprint(response.json()[0])\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"I am sorry for your loss\",\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
